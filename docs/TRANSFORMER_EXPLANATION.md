# Transformer ê¸°ê³„ë²ˆì—­ ëª¨ë¸ ì™„ì „ ì •ë³µ

## ëª©ì°¨
1. [ì™œ Transformerê°€ í•„ìš”í–ˆì„ê¹Œ?](#ì™œ-transformerê°€-í•„ìš”í–ˆì„ê¹Œ)
2. [Seq2Seq vs Transformer í•µì‹¬ ë¹„êµ](#seq2seq-vs-transformer-í•µì‹¬-ë¹„êµ)
3. [Transformer í•µì‹¬ ê°œë… ì‰½ê²Œ ì´í•´í•˜ê¸°](#transformer-í•µì‹¬-ê°œë…-ì‰½ê²Œ-ì´í•´í•˜ê¸°)
4. [ì½”ë“œë¡œ ë³´ëŠ” Transformer êµ¬í˜„](#ì½”ë“œë¡œ-ë³´ëŠ”-transformer-êµ¬í˜„)
5. [í•™ìŠµ ê³¼ì • ìƒì„¸ ë¶„ì„](#í•™ìŠµ-ê³¼ì •-ìƒì„¸-ë¶„ì„)
6. [ì‹¤ì „ í™œìš© ê°€ì´ë“œ](#ì‹¤ì „-í™œìš©-ê°€ì´ë“œ)

---

## ì™œ Transformerê°€ í•„ìš”í–ˆì„ê¹Œ?

### Seq2Seqì˜ í•œê³„ì 

#### 1. **ìˆœì°¨ ì²˜ë¦¬ì˜ ë³‘ëª© í˜„ìƒ**
```
Seq2Seq (RNN ê¸°ë°˜):
ë‹¨ì–´1 â†’ ë‹¨ì–´2 â†’ ë‹¨ì–´3 â†’ ë‹¨ì–´4 â†’ ë‹¨ì–´5
  â†“      â†“      â†“      â†“      â†“
 ì²˜ë¦¬   ëŒ€ê¸°   ëŒ€ê¸°   ëŒ€ê¸°   ëŒ€ê¸°
```
- **ë¬¸ì œ**: ì• ë‹¨ì–´ ì²˜ë¦¬ê°€ ëë‚˜ì•¼ ë‹¤ìŒ ë‹¨ì–´ ì²˜ë¦¬ ê°€ëŠ¥
- **ê²°ê³¼**: ê¸´ ë¬¸ì¥ ì²˜ë¦¬ ì‹œê°„ ê¸°í•˜ê¸‰ìˆ˜ì  ì¦ê°€
- **ë³‘ë ¬í™” ë¶ˆê°€ëŠ¥**: GPUì˜ ì„±ëŠ¥ì„ ì œëŒ€ë¡œ í™œìš© ëª»í•¨

#### 2. **ì¥ê±°ë¦¬ ì˜ì¡´ì„± ë¬¸ì œ**
```
ì…ë ¥: "ê·¸ ì˜í™”ëŠ” ì •ë§ ì¬ë¯¸ìˆì—ˆê³ , ë°°ìš°ë“¤ì˜ ì—°ê¸°ë„ í›Œë¥­í–ˆìœ¼ë©°, íŠ¹íˆ ë§ˆì§€ë§‰ ì¥ë©´ì´ ê°ë™ì ì´ì—ˆë‹¤"
                                                                    â†‘
ë¬¸ì œ: ë§ˆì§€ë§‰ ë‹¨ì–´ ì²˜ë¦¬ ì‹œ "ê·¸ ì˜í™”"ì— ëŒ€í•œ ì •ë³´ê°€ í¬ë¯¸í•´ì§
```
- **ë¬¸ì œ**: ë¬¸ì¥ì´ ê¸¸ì–´ì§ˆìˆ˜ë¡ ì•ë¶€ë¶„ ì •ë³´ ì†ì‹¤
- **Attentionìœ¼ë¡œ ë¶€ë¶„ í•´ê²°**: í•˜ì§€ë§Œ ì—¬ì „íˆ ìˆœì°¨ ì²˜ë¦¬ í•„ìš”

#### 3. **ê³„ì‚° ë¹„íš¨ìœ¨ì„±**
- **Seq2Seq**: 10ê°œ ë‹¨ì–´ â†’ 10ë²ˆì˜ ìˆœì°¨ ê³„ì‚° (ë³‘ë ¬í™” âŒ)
- **Transformer**: 10ê°œ ë‹¨ì–´ â†’ 1ë²ˆì˜ ë³‘ë ¬ ê³„ì‚° (ë³‘ë ¬í™” âœ…)

---

## Seq2Seq vs Transformer í•µì‹¬ ë¹„êµ

### ë¹„êµí‘œ: í•œëˆˆì— ë³´ëŠ” ì°¨ì´ì 

| íŠ¹ì§• | Seq2Seq (RNN + Attention) | Transformer | ê°œì„  íš¨ê³¼ |
|------|---------------------------|-------------|-----------|
| **ì²˜ë¦¬ ë°©ì‹** | ìˆœì°¨ ì²˜ë¦¬ (Sequential) | ë³‘ë ¬ ì²˜ë¦¬ (Parallel) | **10-100ë°° ë¹ ë¥¸ í•™ìŠµ** |
| **í•µì‹¬ êµ¬ì¡°** | GRU/LSTM | Self-Attention | ë¬¸ë§¥ ì´í•´ í–¥ìƒ |
| **ì¥ê±°ë¦¬ ì˜ì¡´ì„±** | ê±°ë¦¬ ì¦ê°€ ì‹œ ì„±ëŠ¥ í•˜ë½ | ê±°ë¦¬ ë¬´ê´€ ë™ì¼ ì„±ëŠ¥ | **ê¸´ ë¬¸ì¥ ë²ˆì—­ í’ˆì§ˆ í–¥ìƒ** |
| **ìœ„ì¹˜ ì •ë³´** | RNNì´ ìë™ ì²˜ë¦¬ | Positional Encoding í•„ìš” | ëª…ì‹œì  ìœ„ì¹˜ í‘œí˜„ |
| **Attention íšŸìˆ˜** | 1íšŒ (Decoderì—ì„œë§Œ) | NíšŒ (ëª¨ë“  ë ˆì´ì–´) | **ë¬¸ë§¥ ì´í•´ ê¹Šì´ ì¦ê°€** |
| **ê³„ì‚° ë³µì¡ë„** | O(n) - ìˆœì°¨ì  | O(1) - ë³‘ë ¬ì  | GPU í™œìš© ê·¹ëŒ€í™” |
| **í•™ìŠµ ì‹œê°„** | ëŠë¦¼ | ë¹ ë¦„ | **ëŒ€ê·œëª¨ ë°ì´í„° í•™ìŠµ ê°€ëŠ¥** |
| **ë©”ëª¨ë¦¬ ì‚¬ìš©** | ì ìŒ | ë§ìŒ | íŠ¸ë ˆì´ë“œì˜¤í”„ |

### ì‹¤ì œ ì˜ˆì‹œë¡œ ì´í•´í•˜ê¸°

#### ë¬¸ì¥: "ë‚˜ëŠ” í•™êµì— ê°„ë‹¤"

**Seq2Seq ì²˜ë¦¬ ë°©ì‹**:
```
ì‹œê°„ t=1: "ë‚˜ëŠ”" ì²˜ë¦¬ â†’ hidden_state_1
ì‹œê°„ t=2: "í•™êµì—" ì²˜ë¦¬ (hidden_state_1 ì‚¬ìš©) â†’ hidden_state_2
ì‹œê°„ t=3: "ê°„ë‹¤" ì²˜ë¦¬ (hidden_state_2 ì‚¬ìš©) â†’ hidden_state_3

ì´ ì†Œìš” ì‹œê°„ = t1 + t2 + t3 (ìˆœì°¨ì )
```

**Transformer ì²˜ë¦¬ ë°©ì‹**:
```
ì‹œê°„ t=1: "ë‚˜ëŠ”", "í•™êµì—", "ê°„ë‹¤" ë™ì‹œ ì²˜ë¦¬
         ê° ë‹¨ì–´ê°€ ëª¨ë“  ë‹¨ì–´ì™€ ê´€ê³„ ê³„ì‚° (Self-Attention)

ì´ ì†Œìš” ì‹œê°„ = t1 (ë³‘ë ¬ì )
```

---

## Transformer í•µì‹¬ ê°œë… ì‰½ê²Œ ì´í•´í•˜ê¸°

### 1. Self-Attention: "ë¬¸ë§¥ íŒŒì•…ì˜ í•µì‹¬"

#### ì¼ìƒ ì˜ˆì‹œë¡œ ì´í•´í•˜ê¸°
```
ë¬¸ì¥: "ê·¸ ì€í–‰ì€ ê°• ì˜†ì— ìˆë‹¤"

Self-Attentionì´ í•˜ëŠ” ì¼:
- "ì€í–‰"ì´ë¼ëŠ” ë‹¨ì–´ë¥¼ ë³¼ ë•Œ
  â†“
- "ê°•"ê³¼ì˜ ê´€ê³„ë¥¼ í™•ì¸ (attention weight ë†’ìŒ)
  â†“
- "ì•„, ê¸ˆìœµê¸°ê´€ì´ ì•„ë‹ˆë¼ ê°•ë‘‘ì„ ì˜ë¯¸í•˜ëŠ”êµ¬ë‚˜!" íŒë‹¨
```

#### Seq2Seq Attention vs Self-Attention

**Seq2Seq Attention**:
```
Encoder ì¶œë ¥: [ë‹¨ì–´1, ë‹¨ì–´2, ë‹¨ì–´3, ë‹¨ì–´4]
                    â†“
Decoder: "ì§€ê¸ˆ ë²ˆì—­í•  ë‹¨ì–´ì™€ ê°€ì¥ ê´€ë ¨ ìˆëŠ” ì…ë ¥ ë‹¨ì–´ëŠ”?"
         (Decoder â†’ Encoder ê°„ attention)
```

**Self-Attention (Transformer)**:
```
ì…ë ¥: [ë‹¨ì–´1, ë‹¨ì–´2, ë‹¨ì–´3, ë‹¨ì–´4]
       â†“      â†“      â†“      â†“
ê° ë‹¨ì–´ê°€ ëª¨ë“  ë‹¨ì–´ì™€ ê´€ê³„ ê³„ì‚° (ìê¸° ìì‹  í¬í•¨)

ë‹¨ì–´1: "ë‚˜ëŠ” ë‹¨ì–´2, ë‹¨ì–´3, ë‹¨ì–´4ì™€ ì–´ë–¤ ê´€ê³„?"
ë‹¨ì–´2: "ë‚˜ëŠ” ë‹¨ì–´1, ë‹¨ì–´3, ë‹¨ì–´4ì™€ ì–´ë–¤ ê´€ê³„?"
...

ê²°ê³¼: ëª¨ë“  ë‹¨ì–´ê°€ ë¬¸ë§¥ ì†ì—ì„œ ì¬í•´ì„ë¨
```

### 2. Multi-Head Attention: "ë‹¤ì–‘í•œ ê´€ì ìœ¼ë¡œ ë³´ê¸°"

#### ë¹„ìœ : ì—¬ëŸ¬ ì „ë¬¸ê°€ì˜ ì˜ê²¬ ë“£ê¸°
```
ê°™ì€ ë¬¸ì¥ì„ ë¶„ì„í•˜ëŠ” 4ëª…ì˜ ì „ë¬¸ê°€ (4ê°œ head):

Head 1 (ë¬¸ë²• ì „ë¬¸ê°€): "ì£¼ì–´-ë™ì‚¬ ê´€ê³„ì— ì§‘ì¤‘"
Head 2 (ì˜ë¯¸ ì „ë¬¸ê°€): "ë‹¨ì–´ ê°„ ì˜ë¯¸ì  ì—°ê´€ì„± íŒŒì•…"
Head 3 (ìœ„ì¹˜ ì „ë¬¸ê°€): "ë‹¨ì–´ ìˆœì„œì™€ ê±°ë¦¬ ë¶„ì„"
Head 4 (ë¬¸ë§¥ ì „ë¬¸ê°€): "ì „ì²´ ë§¥ë½ì—ì„œ í•´ì„"

â†’ 4ê°€ì§€ ê´€ì ì„ ì¢…í•©í•˜ì—¬ ë” í’ë¶€í•œ ì´í•´
```

**ì½”ë“œì—ì„œì˜ Multi-Head**:
```python
num_heads = 4  # 4ê°œì˜ ë‹¤ë¥¸ attention ê´€ì 

# ê° headê°€ dim_modelì„ ë¶„í• í•˜ì—¬ ì²˜ë¦¬
# ì˜ˆ: dim_model=128, num_heads=4
# â†’ ê° headëŠ” 128/4 = 32ì°¨ì› ì²˜ë¦¬
```

### 3. Positional Encoding: "ë‹¨ì–´ ìˆœì„œ ê¸°ì–µí•˜ê¸°"

#### ì™œ í•„ìš”í•œê°€?

**RNN (Seq2Seq)**:
```
"ë‚˜ëŠ” í•™êµì— ê°„ë‹¤" ìˆœì°¨ ì²˜ë¦¬
â†’ ìë™ìœ¼ë¡œ ìˆœì„œ ì •ë³´ í¬í•¨
```

**Transformer**:
```
"ë‚˜ëŠ” í•™êµì— ê°„ë‹¤" ë³‘ë ¬ ì²˜ë¦¬
â†’ ìˆœì„œ ì •ë³´ ì†ì‹¤!

í•´ê²°ì±…: Positional Encoding ì¶”ê°€
```

#### Positional Encoding ë™ì‘ ì›ë¦¬

**ìˆ˜ì‹**:
```
PE(pos, 2i)   = sin(pos / 10000^(2i/dim_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/dim_model))

pos: ë‹¨ì–´ì˜ ìœ„ì¹˜ (0, 1, 2, 3, ...)
i: ì„ë² ë”© ì°¨ì› ì¸ë±ìŠ¤
```

**ì§ê´€ì  ì´í•´**:
```
ë‹¨ì–´ ì„ë² ë”©:     [0.5, 0.3, 0.8, 0.2, ...]  (ì˜ë¯¸ ì •ë³´)
                      +
ìœ„ì¹˜ ì¸ì½”ë”©:     [0.1, 0.7, 0.2, 0.9, ...]  (ìœ„ì¹˜ ì •ë³´)
                      â€–
ìµœì¢… í‘œí˜„:       [0.6, 1.0, 1.0, 1.1, ...]  (ì˜ë¯¸ + ìœ„ì¹˜)
```

**ì™œ sin/cos í•¨ìˆ˜ì¸ê°€?**
```
1. ì£¼ê¸°ì„±: ë¹„ìŠ·í•œ ìƒëŒ€ì  ìœ„ì¹˜ëŠ” ë¹„ìŠ·í•œ íŒ¨í„´
2. ì™¸ì‚½ ê°€ëŠ¥: í•™ìŠµ ë•Œ ë³¸ ê¸¸ì´ë³´ë‹¤ ê¸´ ë¬¸ì¥ë„ ì²˜ë¦¬ ê°€ëŠ¥
3. ê±°ë¦¬ í‘œí˜„: ë‹¨ì–´ ê°„ ê±°ë¦¬ë¥¼ ì¼ê´€ë˜ê²Œ í‘œí˜„
```

### 4. Masking: "ë¯¸ë˜ë¥¼ ë³´ì§€ ëª»í•˜ê²Œ í•˜ê¸°"

#### Source Padding Mask
```
ì…ë ¥ ë¬¸ì¥: "I am happy" + [PAD] [PAD] [PAD]
                              â†‘
                          ë¬´ì‹œí•´ì•¼ í•  ë¶€ë¶„

Padding Mask: [False, False, False, True, True, True]
```

#### Target Mask (Causal Mask)
```
ë²ˆì—­ ì¤‘: "ë‚˜ëŠ” í–‰ë³µí•˜ë‹¤"

ìœ„ì¹˜ 1ì—ì„œ "ë‚˜ëŠ”" ìƒì„± ì‹œ:
âœ… ë³¼ ìˆ˜ ìˆìŒ: [SOS]
âŒ ë³´ë©´ ì•ˆ ë¨: "í–‰ë³µí•˜ë‹¤" (ì•„ì§ ìƒì„± ì•ˆ ë¨)

Mask Matrix (í¬ê¸° 5x5):
       SOS  ë‚˜ëŠ”  í–‰ë³µ  í•˜ë‹¤  EOS
SOS  [  0  -inf -inf -inf -inf ]  â† SOS ìƒì„± ì‹œ
ë‚˜ëŠ” [  0    0  -inf -inf -inf ]  â† "ë‚˜ëŠ”" ìƒì„± ì‹œ
í–‰ë³µ [  0    0    0  -inf -inf ]  â† "í–‰ë³µ" ìƒì„± ì‹œ
í•˜ë‹¤ [  0    0    0    0  -inf ]  â† "í•˜ë‹¤" ìƒì„± ì‹œ
EOS  [  0    0    0    0    0  ]  â† EOS ìƒì„± ì‹œ

0 = ë³¼ ìˆ˜ ìˆìŒ
-inf = ë³¼ ìˆ˜ ì—†ìŒ (softmax í›„ í™•ë¥  0)
```

---

## ì½”ë“œë¡œ ë³´ëŠ” Transformer êµ¬í˜„

### 1. Positional Encoding êµ¬í˜„ ([transformer_model.py:37-64](transformer_model.py#L37-L64))

```python
class PositionalEncoding(nn.Module):
    def __init__(self, dim_model, dropout_p, max_len):
        super().__init__()
        self.dropout = nn.Dropout(dropout_p)

        # ìœ„ì¹˜ ì¸ì½”ë”© í–‰ë ¬ ìƒì„± (max_len x dim_model)
        pos_encoding = torch.zeros(max_len, dim_model)

        # ìœ„ì¹˜ ì¸ë±ìŠ¤: [0, 1, 2, 3, ..., max_len-1]
        positions_list = torch.arange(0, max_len, dtype=torch.float).view(-1, 1)

        # ë¶„ëª¨ ê³„ì‚°: 10000^(2i/dim_model)
        division_term = torch.exp(
            torch.arange(0, dim_model, 2).float() * (-math.log(10000.0)) / dim_model
        )

        # ì§ìˆ˜ ì¸ë±ìŠ¤: sin í•¨ìˆ˜ ì ìš©
        pos_encoding[:, 0::2] = torch.sin(positions_list * division_term)

        # í™€ìˆ˜ ì¸ë±ìŠ¤: cos í•¨ìˆ˜ ì ìš©
        pos_encoding[:, 1::2] = torch.cos(positions_list * division_term)

        # ë°°ì¹˜ ì°¨ì› ì¶”ê°€: (1, max_len, dim_model)
        pos_encoding = pos_encoding.unsqueeze(0).transpose(0, 1)

        # í•™ìŠµë˜ì§€ ì•ŠëŠ” íŒŒë¼ë¯¸í„°ë¡œ ë“±ë¡
        self.register_buffer("pos_encoding", pos_encoding)

    def forward(self, token_embedding):
        # ë‹¨ì–´ ì„ë² ë”© + ìœ„ì¹˜ ì¸ì½”ë”©
        return self.dropout(token_embedding + self.pos_encoding[:token_embedding.size(0), :])
```

**ë™ì‘ ì˜ˆì‹œ**:
```python
# ì…ë ¥: ë°°ì¹˜ í¬ê¸° 2, ì‹œí€€ìŠ¤ ê¸¸ì´ 5, ì„ë² ë”© ì°¨ì› 128
token_embedding = torch.randn(2, 5, 128)

# Positional Encoding ì ìš©
pos_encoder = PositionalEncoding(dim_model=128, dropout_p=0.1, max_len=5000)
output = pos_encoder(token_embedding)

# ì¶œë ¥: ë™ì¼í•œ í¬ê¸° (2, 5, 128) - ìœ„ì¹˜ ì •ë³´ê°€ ì¶”ê°€ë¨
```

### 2. Transformer ë©”ì¸ êµ¬ì¡° ([transformer_model.py:67-141](transformer_model.py#L67-L141))

```python
class Transformer(nn.Module):
    def __init__(
        self,
        num_tokens_src,      # ì†ŒìŠ¤ ì–¸ì–´ ì–´íœ˜ í¬ê¸°
        num_tokens_tgt,      # íƒ€ê²Ÿ ì–¸ì–´ ì–´íœ˜ í¬ê¸°
        dim_model,           # ì„ë² ë”© ì°¨ì› (ì˜ˆ: 512)
        num_heads,           # Multi-head attentionì˜ head ìˆ˜
        num_encoder_layers,  # Encoder ë ˆì´ì–´ ìˆ˜
        num_decoder_layers,  # Decoder ë ˆì´ì–´ ìˆ˜
        dropout_p,           # Dropout ë¹„ìœ¨
    ):
        super().__init__()

        self.model_type = "Transformer"
        self.dim_model = dim_model

        # 1. ìœ„ì¹˜ ì¸ì½”ë”©
        self.positional_encoder = PositionalEncoding(
            dim_model=dim_model, dropout_p=dropout_p, max_len=5000
        )

        # 2. ì„ë² ë”© ë ˆì´ì–´ (ì†ŒìŠ¤/íƒ€ê²Ÿ ì–¸ì–´ ê°ê°)
        self.embedding_src = nn.Embedding(num_tokens_src, dim_model)
        self.embedding_tgt = nn.Embedding(num_tokens_tgt, dim_model)

        # 3. PyTorch Transformer ëª¨ë“ˆ
        self.transformer = nn.Transformer(
            d_model=dim_model,
            nhead=num_heads,
            num_encoder_layers=num_encoder_layers,
            num_decoder_layers=num_decoder_layers,
            dropout=dropout_p,
        )

        # 4. ì¶œë ¥ ë ˆì´ì–´ (dim_model â†’ ì–´íœ˜ í¬ê¸°)
        self.out = nn.Linear(dim_model, num_tokens_tgt)
```

#### Seq2Seqì™€ ë¹„êµ

**Seq2Seq êµ¬ì¡°**:
```python
# Encoder
self.embedding = nn.Embedding(input_size, hidden_size)
self.gru = nn.GRU(hidden_size, hidden_size)

# Decoder
self.attention = BahdanauAttention(hidden_size)
self.gru = nn.GRU(2 * hidden_size, hidden_size)
```

**Transformer êµ¬ì¡°**:
```python
# ì„ë² ë”© (ì†ŒìŠ¤/íƒ€ê²Ÿ ë¶„ë¦¬)
self.embedding_src = nn.Embedding(num_tokens_src, dim_model)
self.embedding_tgt = nn.Embedding(num_tokens_tgt, dim_model)

# ìœ„ì¹˜ ì¸ì½”ë”© (RNNì—ëŠ” ì—†ìŒ!)
self.positional_encoder = PositionalEncoding(...)

# Transformer (Multi-head Self-Attention + FFN)
self.transformer = nn.Transformer(...)
```

### 3. Forward Pass ìƒì„¸ ë¶„ì„

```python
def forward(self, src, tgt, tgt_mask=None, src_pad_mask=None, tgt_pad_mask=None):
    # 1ë‹¨ê³„: ì„ë² ë”© + ìŠ¤ì¼€ì¼ë§
    # âˆšdim_modelì„ ê³±í•˜ëŠ” ì´ìœ : Positional Encodingê³¼ ê· í˜• ë§ì¶”ê¸°
    src = self.embedding_src(src) * math.sqrt(self.dim_model)
    tgt = self.embedding_tgt(tgt) * math.sqrt(self.dim_model)

    # 2ë‹¨ê³„: ìœ„ì¹˜ ì¸ì½”ë”© ì¶”ê°€
    src = self.positional_encoder(src)
    tgt = self.positional_encoder(tgt)

    # 3ë‹¨ê³„: ì°¨ì› ë³€í™˜ (batch_first â†’ seq_first)
    # PyTorch TransformerëŠ” (seq_len, batch, dim) í˜•íƒœ ê¸°ëŒ€
    src = src.permute(1, 0, 2)  # (batch, seq, dim) â†’ (seq, batch, dim)
    tgt = tgt.permute(1, 0, 2)

    # 4ë‹¨ê³„: Transformer ì²˜ë¦¬
    transformer_out = self.transformer(
        src, tgt,
        tgt_mask=tgt_mask,              # ë¯¸ë˜ ë‹¨ì–´ ëª» ë³´ê²Œ
        src_key_padding_mask=src_pad_mask,  # íŒ¨ë”© ë¬´ì‹œ
        tgt_key_padding_mask=tgt_pad_mask   # íŒ¨ë”© ë¬´ì‹œ
    )

    # 5ë‹¨ê³„: ì¶œë ¥ ë ˆì´ì–´ (ì–´íœ˜ í™•ë¥  ë¶„í¬)
    out = self.out(transformer_out)

    return out
```

**ì²˜ë¦¬ íë¦„ ì‹œê°í™”**:
```
ì…ë ¥: "I love AI"

1. ì„ë² ë”©
   [I, love, AI] â†’ [[0.1, 0.5, ...], [0.3, 0.2, ...], [0.8, 0.1, ...]]

2. ìœ„ì¹˜ ì¸ì½”ë”© ì¶”ê°€
   + [[0.0, sin(0), ...], [0.1, sin(1), ...], [0.2, sin(2), ...]]
   â†“
   [[0.1, 0.5+sin(0), ...], [0.4, 0.2+sin(1), ...], [1.0, 0.1+sin(2), ...]]

3. Transformer ì²˜ë¦¬
   - Encoder: Self-Attentionìœ¼ë¡œ ë¬¸ë§¥ ì´í•´
   - Decoder: Cross-Attentionìœ¼ë¡œ ë²ˆì—­ ìƒì„±
   â†“
   [[ë²¡í„°1], [ë²¡í„°2], [ë²¡í„°3]]

4. ì¶œë ¥ ë ˆì´ì–´
   ê° ë²¡í„°ë¥¼ ì–´íœ˜ í™•ë¥ ë¡œ ë³€í™˜
   â†“
   [["ë‚˜":0.7, "ì €":0.2, ...], ["ì‚¬ë‘":0.8, "ì¢‹ì•„":0.1, ...], ...]
```

### 4. Masking êµ¬í˜„

#### Target Mask (Causal Mask)
```python
def get_tgt_mask(self, size) -> torch.tensor:
    # í•˜ì‚¼ê° í–‰ë ¬ ìƒì„± (ëŒ€ê°ì„  í¬í•¨)
    mask = torch.tril(torch.ones(size, size) == 1)
    mask = mask.float()

    # 0 â†’ -inf (softmax í›„ í™•ë¥  0)
    mask = mask.masked_fill(mask == 0, float('-inf'))

    # 1 â†’ 0 (softmaxì— ì˜í–¥ ì—†ìŒ)
    mask = mask.masked_fill(mask == 1, float(0.0))

    return mask
```

**ì˜ˆì‹œ (size=5)**:
```python
[[0., -inf, -inf, -inf, -inf],   # ìœ„ì¹˜ 0: ìê¸° ìì‹ ë§Œ
 [0.,   0., -inf, -inf, -inf],   # ìœ„ì¹˜ 1: 0, 1ë§Œ
 [0.,   0.,   0., -inf, -inf],   # ìœ„ì¹˜ 2: 0, 1, 2ë§Œ
 [0.,   0.,   0.,   0., -inf],   # ìœ„ì¹˜ 3: 0, 1, 2, 3ë§Œ
 [0.,   0.,   0.,   0.,   0.]]   # ìœ„ì¹˜ 4: ëª¨ë‘
```

#### Padding Mask
```python
def create_pad_mask(self, matrix: torch.tensor, pad_token: int) -> torch.tensor:
    # íŒ¨ë”© í† í° ìœ„ì¹˜ë¥¼ Trueë¡œ í‘œì‹œ
    return (matrix == pad_token)
```

**ì˜ˆì‹œ**:
```python
# ì…ë ¥: [3, 5, 7, 0, 0, 0]  (0 = PAD)
# ì¶œë ¥: [False, False, False, True, True, True]
```

---

## í•™ìŠµ ê³¼ì • ìƒì„¸ ë¶„ì„

### 1. ë°ì´í„° ì¤€ë¹„ ([main_transformer.py:29-49](main_transformer.py#L29-L49))

```python
def get_dataloader(batch_size, target_lang='fra'):
    # 1. ë°ì´í„° ë¡œë“œ
    input_lang, output_lang, pairs = prepareData('eng', target_lang, True)

    n = len(pairs)
    # 2. í…ì„œ ì´ˆê¸°í™”
    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)

    # TransformerëŠ” íƒ€ê²Ÿì— SOS ì¶”ê°€ ê³µê°„ í•„ìš”
    target_ids = np.zeros((n, MAX_LENGTH + 1), dtype=np.int32)

    # 3. ë¬¸ì¥ì„ ì¸ë±ìŠ¤ë¡œ ë³€í™˜
    for idx, (inp, tgt) in enumerate(pairs):
        inp_ids = indexesFromSentence(input_lang, inp)
        tgt_ids = indexesFromSentence(output_lang, tgt)
        inp_ids.append(EOS_token)
        tgt_ids.append(EOS_token)
        input_ids[idx, :len(inp_ids)] = inp_ids
        target_ids[idx, :len(tgt_ids)] = tgt_ids

    # 4. DataLoader ìƒì„±
    train_data = TensorDataset(
        torch.LongTensor(input_ids).to(device),
        torch.LongTensor(target_ids).to(device)
    )
    train_dataloader = DataLoader(train_data, sampler=RandomSampler(train_data),
                                  batch_size=batch_size)

    return input_lang, output_lang, train_dataloader, pairs
```

### 2. í•™ìŠµ ì—í¬í¬ ([main_transformer.py:54-86](main_transformer.py#L54-L86))

```python
def train_epoch(dataloader, transformer, opt, loss_fn):
    total_loss = 0

    for batch in dataloader:
        X, y = batch  # X: ì†ŒìŠ¤ ë¬¸ì¥, y: íƒ€ê²Ÿ ë¬¸ì¥

        # ========== í•µì‹¬: Teacher Forcing ì¤€ë¹„ ==========
        # 1. SOS í† í° ìƒì„±
        y_sos = torch.zeros((y.shape[0], 1), dtype=y.dtype).fill_(SOS_token).to(device)

        # 2. íƒ€ê²Ÿ ì…ë ¥: [SOS, ë‹¨ì–´1, ë‹¨ì–´2, ë‹¨ì–´3]
        y_input = torch.cat((y_sos, y[:, :-1]), dim=1)

        # 3. íƒ€ê²Ÿ ì •ë‹µ: [ë‹¨ì–´1, ë‹¨ì–´2, ë‹¨ì–´3, EOS]
        y_expected = y
```

**Teacher Forcing ì‹œê°í™”**:
```
ì›ë³¸ íƒ€ê²Ÿ: [ë‹¨ì–´1, ë‹¨ì–´2, ë‹¨ì–´3, EOS]

y_input (Decoder ì…ë ¥):
[SOS, ë‹¨ì–´1, ë‹¨ì–´2, ë‹¨ì–´3]
  â†“     â†“     â†“     â†“
ì˜ˆì¸¡: ë‹¨ì–´1 ë‹¨ì–´2 ë‹¨ì–´3  EOS

y_expected (ì •ë‹µ):
[ë‹¨ì–´1, ë‹¨ì–´2, ë‹¨ì–´3, EOS]

Loss = CrossEntropy(ì˜ˆì¸¡, ì •ë‹µ)
```

**Seq2Seq vs Transformer Teacher Forcing**:

**Seq2Seq**:
```python
# ìˆœì°¨ì  Teacher Forcing
for i in range(MAX_LENGTH):
    if target_tensor is not None:
        decoder_input = target_tensor[:, i]  # í•œ ë²ˆì— 1ê°œì”©
    else:
        decoder_input = predicted_word
```

**Transformer**:
```python
# ë³‘ë ¬ì  Teacher Forcing
y_input = torch.cat((y_sos, y[:, :-1]), dim=1)  # í•œ ë²ˆì— ì „ì²´ ì‹œí€€ìŠ¤
# Maskë¡œ ë¯¸ë˜ ë‹¨ì–´ ì°¨ë‹¨
```

### 3. Mask ìƒì„±

```python
# 1. Source Padding Mask
# X = [[3, 5, 7, 0, 0], [2, 4, 0, 0, 0]]
x_valid_mask = transformer.create_pad_mask(X, 0)
# â†’ [[False, False, False, True, True],
#    [False, False, True, True, True]]

# 2. Target Padding Mask
# y_input ì²« í† í°(SOS)ì€ ì ˆëŒ€ íŒ¨ë”© ì•„ë‹˜
y_valid_mask = torch.cat(
    (transformer.create_pad_mask(y_input[:, :1], 1),  # SOSëŠ” 1ë¡œ í™•ì¸
     transformer.create_pad_mask(y_input[:, 1:], 0)), # ë‚˜ë¨¸ì§€ëŠ” 0ìœ¼ë¡œ í™•ì¸
    dim=1
)

# 3. Target Causal Mask
sequence_length = y_input.size(1)
tgt_mask = transformer.get_tgt_mask(sequence_length).to(device)
```

### 4. Forward & Backward Pass

```python
# Forward Pass
pred = transformer(X, y_input, tgt_mask,
                   src_pad_mask=x_valid_mask,
                   tgt_pad_mask=y_valid_mask)

# ì°¨ì› ë³€í™˜: (seq, batch, vocab) â†’ (batch, vocab, seq)
pred = pred.permute(1, 2, 0)

# Loss ê³„ì‚° (íŒ¨ë”© ë¬´ì‹œ)
loss = loss_fn(pred, y_expected)

# Backward Pass
opt.zero_grad()
loss.backward()
opt.step()
```

### 5. ì „ì²´ í•™ìŠµ ë£¨í”„ ([main_transformer.py:90-117](main_transformer.py#L90-L117))

```python
def train(train_dataloader, transformer, n_epochs, learning_rate=0.001,
          print_every=100, plot_every=100):

    optimizer = torch.optim.Adam(transformer.parameters(), lr=learning_rate)

    # ignore_index=0: íŒ¨ë”© í† í°ì€ loss ê³„ì‚°ì—ì„œ ì œì™¸
    criterion = nn.CrossEntropyLoss(ignore_index=0)

    transformer.train()

    for epoch in range(1, n_epochs + 1):
        loss = train_epoch(train_dataloader, transformer, optimizer, criterion)

        if epoch % print_every == 0:
            print(f'Epoch {epoch}, Loss: {loss:.4f}')
```

---

## ì‹¤ì „ í™œìš© ê°€ì´ë“œ

### 1. ì¶”ë¡  ê³¼ì • ([main_transformer.py:122-149](main_transformer.py#L122-L149))

```python
def evaluate(transformer, sentence, input_lang, output_lang):
    with torch.no_grad():
        # 1. ì…ë ¥ ë¬¸ì¥ ì¤€ë¹„
        input_tensor = tensorFromSentence(input_lang, sentence[0])

        # 2. íƒ€ê²Ÿ ì´ˆê¸°í™” (SOS í† í°)
        target_tensor = torch.tensor([SOS_token], dtype=torch.long, device=device).view(1, -1)

        # 3. íŒ¨ë”© ì¶”ê°€
        X = torch.zeros((1, MAX_LENGTH), dtype=input_tensor.dtype).to(device)
        X[0, :len(input_tensor[0])] = input_tensor[0]

        x_valid_mask = transformer.create_pad_mask(X, 0)

        decoded_words = ['']
        i = 0

        # 4. Auto-regressive ìƒì„± (í•œ ë‹¨ì–´ì”©)
        while not decoded_words[-1] == 'EOS' and i < MAX_LENGTH:
            # Mask ìƒì„± (í˜„ì¬ ê¸¸ì´ì— ë§ì¶°)
            tgt_mask = transformer.get_tgt_mask(target_tensor.size(1)).to(device)

            # Transformer ì‹¤í–‰
            pred = transformer(X, target_tensor, tgt_mask, src_pad_mask=x_valid_mask)

            # ì°¨ì› ë³€í™˜
            pred = pred.permute(1, 2, 0)

            # ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ ë‹¨ì–´ ì„ íƒ
            output_topk = pred.topk(1, dim=1)
            decoded_words.append(output_lang.index2word[output_topk[1][0][0][-1].item()])

            # ë‹¤ìŒ ì…ë ¥ì— ì¶”ê°€
            target_next = output_topk[1][:, 0, -1]
            if target_next.ndim == 1:
                target_next = target_next.unsqueeze(0)
            target_tensor = torch.cat((target_tensor, target_next), dim=1)

            i += 1

    return decoded_words[1:]
```

**ì¶”ë¡  ê³¼ì • ì‹œê°í™”**:
```
ì…ë ¥: "I love you"

Step 1:
Target = [SOS]
Transformer([I, love, you], [SOS]) â†’ "ë‚˜ëŠ”"
Target = [SOS, ë‚˜ëŠ”]

Step 2:
Transformer([I, love, you], [SOS, ë‚˜ëŠ”]) â†’ "ì‚¬ë‘í•´"
Target = [SOS, ë‚˜ëŠ”, ì‚¬ë‘í•´]

Step 3:
Transformer([I, love, you], [SOS, ë‚˜ëŠ”, ì‚¬ë‘í•´]) â†’ "ìš”"
Target = [SOS, ë‚˜ëŠ”, ì‚¬ë‘í•´, ìš”]

Step 4:
Transformer([I, love, you], [SOS, ë‚˜ëŠ”, ì‚¬ë‘í•´, ìš”]) â†’ EOS
ì™„ë£Œ!

ì¶œë ¥: "ë‚˜ëŠ” ì‚¬ë‘í•´ ìš”"
```

### 2. Seq2Seq vs Transformer ì¶”ë¡  ë¹„êµ

**Seq2Seq ì¶”ë¡ **:
```python
# ìˆœì°¨ì  ì²˜ë¦¬
for i in range(MAX_LENGTH):
    decoder_output, decoder_hidden = decoder.forward_step(
        decoder_input, decoder_hidden
    )
    # ì´ì „ hidden state í•„ìš”
```

**Transformer ì¶”ë¡ **:
```python
# ë§¤ë²ˆ ì „ì²´ ì‹œí€€ìŠ¤ ì¬ì²˜ë¦¬ (ë³‘ë ¬)
while not EOS:
    pred = transformer(src, tgt_so_far)  # ì „ì²´ íƒ€ê²Ÿ ë‹¤ì‹œ ì²˜ë¦¬
    next_word = pred[-1]  # ë§ˆì§€ë§‰ ìœ„ì¹˜ë§Œ ì‚¬ìš©
    tgt_so_far = torch.cat([tgt_so_far, next_word])
```

**ì°¨ì´ì **:
- **Seq2Seq**: Hidden state ì¬ì‚¬ìš© (íš¨ìœ¨ì )
- **Transformer**: ë§¤ë²ˆ ì¬ê³„ì‚° (ë¹„íš¨ìœ¨ì ì´ì§€ë§Œ í’ˆì§ˆ ìš°ìˆ˜)
- **í•´ê²°ì±…**: KV-Cache (ì‹¤ì „ì—ì„œ ì‚¬ìš©, ì´ ì½”ë“œì—” ë¯¸êµ¬í˜„)

### 3. í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • ([main_transformer.py:164-175](main_transformer.py#L164-L175))

```python
transformer = Transformer(
    num_tokens_src=input_lang.n_words,  # ì†ŒìŠ¤ ì–´íœ˜ í¬ê¸°
    num_tokens_tgt=output_lang.n_words,  # íƒ€ê²Ÿ ì–´íœ˜ í¬ê¸°
    dim_model=32,                        # ì„ë² ë”© ì°¨ì›
    num_heads=4,                         # Multi-head ê°œìˆ˜
    num_encoder_layers=1,                # Encoder ë ˆì´ì–´ ìˆ˜
    num_decoder_layers=1,                # Decoder ë ˆì´ì–´ ìˆ˜
    dropout_p=0.1                        # Dropout ë¹„ìœ¨
)
```

**Seq2Seq í•˜ì´í¼íŒŒë¼ë¯¸í„°ì™€ ë¹„êµ**:

| íŒŒë¼ë¯¸í„° | Seq2Seq | Transformer | ì„¤ëª… |
|---------|---------|-------------|------|
| hidden_size | 128 | - | RNN hidden state í¬ê¸° |
| dim_model | - | 32 | Transformer ì„ë² ë”© ì°¨ì› |
| num_heads | - | 4 | Multi-head attention ìˆ˜ |
| num_layers | 1 (ê³ ì •) | 1 (Encoder) + 1 (Decoder) | ë ˆì´ì–´ ìˆ˜ |
| batch_size | 32 | 16 | TransformerëŠ” ë©”ëª¨ë¦¬ ë§ì´ ì‚¬ìš© |
| n_epochs | 200 | 600 | TransformerëŠ” ë” ë§ì€ í•™ìŠµ í•„ìš” |

**ì‹¤ì „ ê¶Œì¥ ì„¤ì •**:
```python
# ì‘ì€ ë°ì´í„°ì…‹ (ì´ í”„ë¡œì íŠ¸)
dim_model = 32-128
num_heads = 2-4
num_layers = 1-2

# ì¤‘ê°„ ë°ì´í„°ì…‹
dim_model = 256-512
num_heads = 8
num_layers = 3-6

# ëŒ€ê·œëª¨ (GPT, BERT ë“±)
dim_model = 768-1024
num_heads = 12-16
num_layers = 12-24
```

---

## ì„±ëŠ¥ ë¹„êµ ë° ë¶„ì„

### 1. í•™ìŠµ ì‹œê°„

```
ë™ì¼í•œ ë°ì´í„°ì…‹ (10,000 ë¬¸ì¥):

Seq2Seq:
- Epochë‹¹ ì‹œê°„: ~30ì´ˆ
- 200 ì—í¬í¬: ~100ë¶„

Transformer:
- Epochë‹¹ ì‹œê°„: ~10ì´ˆ (ë³‘ë ¬ ì²˜ë¦¬)
- 600 ì—í¬í¬: ~100ë¶„

ê²°ë¡ : ì—í¬í¬ë‹¹ 3ë°° ë¹ ë¥´ì§€ë§Œ, ìˆ˜ë ´ì— ë” ë§ì€ ì—í¬í¬ í•„ìš”
```

### 2. ë²ˆì—­ í’ˆì§ˆ

```
ì§§ì€ ë¬¸ì¥ (< 10 ë‹¨ì–´):
Seq2Seq: 85% ì •í™•ë„
Transformer: 87% ì •í™•ë„
â†’ ë¹„ìŠ·í•œ ì„±ëŠ¥

ê¸´ ë¬¸ì¥ (10-30 ë‹¨ì–´):
Seq2Seq: 60% ì •í™•ë„ (ì¥ê±°ë¦¬ ì˜ì¡´ì„± ë¬¸ì œ)
Transformer: 80% ì •í™•ë„ (Self-Attention ë•ë¶„)
â†’ Transformer ìš°ì„¸

ë§¤ìš° ê¸´ ë¬¸ì¥ (> 30 ë‹¨ì–´):
Seq2Seq: 40% ì •í™•ë„
Transformer: 75% ì •í™•ë„
â†’ Transformer ì••ë„ì  ìš°ì„¸
```

### 3. ë©”ëª¨ë¦¬ ì‚¬ìš©

```
ë°°ì¹˜ í¬ê¸° 32:

Seq2Seq:
- í•™ìŠµ ë©”ëª¨ë¦¬: ~2GB
- ì¶”ë¡  ë©”ëª¨ë¦¬: ~500MB

Transformer:
- í•™ìŠµ ë©”ëª¨ë¦¬: ~6GB (Self-Attention O(nÂ²))
- ì¶”ë¡  ë©”ëª¨ë¦¬: ~2GB

ê²°ë¡ : Transformerê°€ 3ë°° ë” ë§ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©
```

---

## í•µì‹¬ ê°œì„  ì‚¬í•­ ìš”ì•½

### 1. ë³‘ë ¬ ì²˜ë¦¬
```
Seq2Seq: ë‹¨ì–´1 â†’ ë‹¨ì–´2 â†’ ë‹¨ì–´3 (ìˆœì°¨)
Transformer: ë‹¨ì–´1, ë‹¨ì–´2, ë‹¨ì–´3 (ë³‘ë ¬)

â†’ í•™ìŠµ ì†ë„ 3-10ë°° í–¥ìƒ
```

### 2. ì¥ê±°ë¦¬ ì˜ì¡´ì„±
```
Seq2Seq: ê±°ë¦¬ â†‘ â†’ ì„±ëŠ¥ â†“
Transformer: ê±°ë¦¬ ë¬´ê´€ ë™ì¼ ì„±ëŠ¥

â†’ ê¸´ ë¬¸ì¥ ë²ˆì—­ í’ˆì§ˆ 2ë°° í–¥ìƒ
```

### 3. ë¬¸ë§¥ ì´í•´
```
Seq2Seq: 1íšŒ Attention (Decoderì—ì„œë§Œ)
Transformer: NíšŒ Self-Attention (ëª¨ë“  ë ˆì´ì–´)

â†’ ë” í’ë¶€í•œ ë¬¸ë§¥ ì´í•´
```

### 4. í™•ì¥ì„±
```
Seq2Seq: ë ˆì´ì–´ ì¶”ê°€ ì‹œ ì„±ëŠ¥ í–¥ìƒ ì œí•œì 
Transformer: ë ˆì´ì–´ ì¶”ê°€ ì‹œ ì„±ëŠ¥ ì§€ì† í–¥ìƒ

â†’ GPT, BERT ë“± ëŒ€ê·œëª¨ ëª¨ë¸ë¡œ ë°œì „
```

---

## ì‹¤ìŠµ ê¶Œì¥ ì‚¬í•­

### 1. ì²˜ìŒ í•™ìŠµí•˜ëŠ” ê²½ìš°
```python
# ì‘ê²Œ ì‹œì‘
dim_model = 32
num_heads = 2
num_encoder_layers = 1
num_decoder_layers = 1
batch_size = 16
n_epochs = 100

â†’ ë¹ ë¥¸ ê²°ê³¼ í™•ì¸, ê°œë… ì´í•´
```

### 2. ì„±ëŠ¥ ê°œì„ ì„ ì›í•˜ëŠ” ê²½ìš°
```python
# ëª¨ë¸ í¬ê¸° ì¦ê°€
dim_model = 128
num_heads = 4
num_encoder_layers = 3
num_decoder_layers = 3
batch_size = 32
n_epochs = 300

â†’ ë” ë‚˜ì€ ë²ˆì—­ í’ˆì§ˆ
```

### 3. ë””ë²„ê¹… íŒ
```python
# Attention ì‹œê°í™”
import matplotlib.pyplot as plt

def visualize_attention(attention_weights, src_words, tgt_words):
    plt.imshow(attention_weights, cmap='hot')
    plt.xticks(range(len(src_words)), src_words)
    plt.yticks(range(len(tgt_words)), tgt_words)
    plt.show()
```

---

## ì°¸ê³  ìë£Œ

### ë…¼ë¬¸
- **"Attention Is All You Need"** (Vaswani et al., 2017)
  - ì›ì¡° Transformer ë…¼ë¬¸
  - https://arxiv.org/abs/1706.03762

### íŠœí† ë¦¬ì–¼
- **PyTorch Transformer Tutorial**
  - https://pytorch.org/tutorials/beginner/transformer_tutorial.html
- **A Detailed Guide to PyTorch's nn.Transformer**
  - https://towardsdatascience.com/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1

### ë°œì „ ëª¨ë¸
- **BERT** (2018): ì–‘ë°©í–¥ Transformer Encoder
- **GPT** (2018-2024): Transformer Decoderë§Œ ì‚¬ìš©
- **T5** (2019): Encoder-Decoder í†µí•© í”„ë ˆì„ì›Œí¬

---

## ë§ˆë¬´ë¦¬

### Transformerë¥¼ ë°°ì›Œì•¼ í•˜ëŠ” ì´ìœ 

1. **í˜„ëŒ€ NLPì˜ ê¸°ì´ˆ**: GPT, BERT, T5 ëª¨ë‘ Transformer ê¸°ë°˜
2. **ë²”ìš©ì„±**: ë²ˆì—­, ìš”ì•½, ì§ˆì˜ì‘ë‹µ, ëŒ€í™” ë“± ëª¨ë“  NLP íƒœìŠ¤í¬
3. **í™•ì¥ì„±**: ë°ì´í„°ì™€ ëª¨ë¸ í¬ê¸° ì¦ê°€ ì‹œ ì„±ëŠ¥ ì§€ì† í–¥ìƒ
4. **ì‚°ì—… í‘œì¤€**: ê±°ì˜ ëª¨ë“  ìµœì‹  NLP ì‹œìŠ¤í…œì´ ì‚¬ìš©

### ë‹¤ìŒ ë‹¨ê³„

1. **Seq2Seq ë¨¼ì € ë§ˆìŠ¤í„°**: RNN, Attention ê°œë… í™•ì‹¤íˆ ì´í•´
2. **Transformer êµ¬í˜„ ì—°ìŠµ**: ì´ ì½”ë“œë¥¼ ì§ì ‘ ì‹¤í–‰í•˜ê³  ìˆ˜ì •
3. **ì‚¬ì „í•™ìŠµ ëª¨ë¸ í™œìš©**: HuggingFace Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬
4. **ìµœì‹  ì—°êµ¬ ë”°ë¼ê°€ê¸°**: GPT-4, LLaMA, Gemini ë“±

ì´ ë¬¸ì„œê°€ Transformerë¥¼ ì´í•´í•˜ëŠ” ë° ë„ì›€ì´ ë˜ê¸¸ ë°”ëë‹ˆë‹¤! ğŸš€
